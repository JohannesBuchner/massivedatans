#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass mnras
\begin_preamble
\pdfminorversion=4
\end_preamble
\options a4paper,fleqn,usenatbib,unicode=true
\use_default_options true
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "unicode=true"
\papersize a4paper
\use_geometry false
\use_package amsmath 2
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\rightmargin 5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
title[Big Data vs complex physical models]{
\end_layout

\end_inset

Big Data vs.
 complex physical models: a scalable inference algorithm 
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout


\backslash
author[J.
 Buchner]{
\end_layout

\begin_layout Plain Layout

J.
 Buchner$^{1,2,3}$
\backslash
thanks{E-mail: johannes.buchner.acad@gmx.com}
\end_layout

\begin_layout Plain Layout


\backslash

\backslash

\end_layout

\begin_layout Plain Layout

$^{1}$Millenium Institute of Astrophysics, Vicu
\backslash
~{n}a.
 MacKenna 4860, 7820436 Macul, Santiago, Chile
\end_layout

\begin_layout Plain Layout


\backslash

\backslash

\end_layout

\begin_layout Plain Layout

$^{2}$Pontificia Universidad Católica de Chile, Instituto de Astrofísica,
 Casilla 306, Santiago 22, Chile
\end_layout

\begin_layout Plain Layout


\backslash

\backslash

\end_layout

\begin_layout Plain Layout

$^{3}$Excellence Cluster Universe, Boltzmannstr.
 2, D-85748, Garching, Germany
\end_layout

\begin_layout Plain Layout

}{
\end_layout

\end_inset


\end_layout

\begin_layout Date
Accepted XXX.
 Received YYY; in original form ZZZ
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout


\backslash
pubyear{2017}
\end_layout

\begin_layout Plain Layout


\backslash
label{firstpage}
\end_layout

\begin_layout Plain Layout


\backslash
pagerange{
\backslash
pageref{firstpage}--
\backslash
pageref{lastpage}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{abstract}
\end_layout

\end_inset

The data torrent unleashed by current and upcoming instruments requires
 scalable analysis methods.
 Machine Learning approaches scale well.
 However, separating the instrument measurement errors from the physical
 effects of interest, dealing with variable errors, and deriving parameter
 uncertainties is usually an after-thought.
 Classic forward-folding analyses with Markov Chain Monte Carlo or Nested
 Sampling enable parameter estimation and model comparison, even for complex
 and slow-to-evaluate physical models.
 However, these approaches require independent runs for each data set, implying
 a large number of model evaluations in the Big Data regime.
 Here we present a new algorithm based on nested sampling, deriving parameter
 probability distributions for each observation.
 Importantly, in our method the number of physical model evaluations scales
 sub-linearly with the number of data sets, and we make no assumptions about
 homogeneous errors, Gaussianity, the form of the model or heterogeneity/complet
eness of the data set.
 Our method has immediate application to speed up analyses of large surveys,
 integral-field-unit analyses, and Monte Carlo simulated data.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{abstract}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

%
\backslash
begin{keywords}
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout

%
\backslash
end{keywords}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Big Data has arrived in astronomy.
 In the previous century it was common to analysing a few dozen objects
 in detail.
 For instance, one would use Markov Chain Monte Carlo to forward fold a
 physical model and constrain its parameters.
 This would be repeated for each member of the sample.
 However, current and upcoming instruments provide a wealth of data (
\begin_inset Formula $\sim$
\end_inset

millions of independent sources) were it becomes computationally difficult
 to follow the same approach, even though it is embarrassingly parallel.
 Currently, much effort is put into studying and applying machine learning
 algorithms such as (deep learning) neural networks for the analysis of
 massive datasets.
 This can work well if the measurement errors are homogeneous, but typically
 these method make it difficult to insert existing physical knowledge into
 the analysis, to deal with variable errors and missing data points, and
 generally to separate the instrument measurement process from the physical
 effects of interest.
 Furthermore, we would like to derive probability density distributions
 of physical parameters for each object, and do model comparison between
 physical effects and/or distinguishing classes of sources.
\end_layout

\begin_layout Standard
In this work I show how nested sampling can be used to analyse 
\begin_inset Formula $N$
\end_inset

 data sets simultaneously.
 I assume that the model can be split into two components: a slow-to-evaluate
 physical model which performs a prediction into observable space, and a
 fast-to-compute comparison to the individual data sets (e.g.
 the likelihood of a probability distribution).
 The key insight is that nested sampling allows effective sharing of evaluation
 points across data sets, requiring much fewer model evaluations than if
 the 
\begin_inset Formula $N$
\end_inset

 data sets were analysed individually.
\end_layout

\begin_layout Standard
The user is free to chose arbitrary physical models and likelihoods.
 In the presented example in §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Results"

\end_inset

 I present line fitting of a hypothetical many-object spectroscopic survey.
 A more advanced example could include broadened H/O/C line emissions from
 various ionisation states under red noise errors, without modification
 of our algorithm.
\end_layout

\begin_layout Section
Methodology
\end_layout

\begin_layout Subsection
Introduction to Classic Nested Sampling
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename plotcontour_1.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename plotcontour_3.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename plotcontour_5.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename plotcontour_6.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:IllustrationNS"

\end_inset

Illustration of nested sampling.
 At a given iteration of the nested sampling algorithm, the live points
 (black) trace out the current likelihood constraint, a region (dashed)
 which is unknown.
 The 
\shape smallcaps
RadFriends
\shape default
 algorithm conservatively reconstructs the region (orange) by including
 everything within a certain, adaptively chosen radius of the current live
 points.
 Between iterations, the likelihood contour is elevated, making the sampled
 volume smaller and smaller.
\end_layout

\end_inset


\end_layout

\end_inset

Nested sampling 
\begin_inset CommandInset citation
LatexCommand citep
key "Skilling2004"

\end_inset

 is a global algorithm on the parameter space, which zooms in from the entire
 volume towards the best-fit models by steadily increasing the likelihood
 threshold.
 In the process, it produces parameter posterior probability distributions
 and computes the integral over the parameter space.
 Assume that the parameter space is a 
\begin_inset Formula $k$
\end_inset

-dimensional cube.
 A number of live points 
\begin_inset Formula $N_{{\rm live}}$
\end_inset

 are randomly
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
In general, following the prior.
 For most problems one can assume uniform sampling with appropriate stretching
 of the parameter space under the inverse cumulative of the prior distributions.
\end_layout

\end_inset

 placed in the parameter space.
 Their likelihood is evaluated.
 Each point represents 
\begin_inset Formula $1/N_{{\rm live}}$
\end_inset

 of the entire volume.
 The live point with the lowest likelihood 
\begin_inset Formula $L_{{\rm min}}$
\end_inset

 is then removed, implying the removal of space with likelihood below 
\begin_inset Formula $L_{{\rm min}}$
\end_inset

 and shrinkage of the volume to 
\begin_inset Formula $(N_{{\rm live}}-1)/N_{{\rm live}}$
\end_inset

, on average.
 A new random live point is drawn, with the requirement that its likelihood
 must be above 
\begin_inset Formula $L_{{\rm min}}$
\end_inset

.
 This replacement procedure is iterated, shrinking the volume exponentially
 through nested sampling.
 Each removed (
\begin_inset Quotes eld
\end_inset

dead
\begin_inset Quotes erd
\end_inset

) point and its likelihood 
\begin_inset Formula $L_{i}$
\end_inset

 is stored.
 The integral over the parameter space can then be approximated by 
\begin_inset Formula $Z=\sum_{i}L_{i}\times w_{i}$
\end_inset

, where 
\begin_inset Formula $w_{i}$
\end_inset

 is the removed volume at the iteration.
 At a late stage in the algorithm the volume probed is tiny and the likelihood
 
\begin_inset Formula $L_{i}$
\end_inset

 increase is negligible, so that the weights 
\begin_inset Formula $L_{i}\times w_{i}$
\end_inset

 of the remaining live points becomes small.
 Then the iterative procedure can be stopped (the algorithm converged).
 The posterior probability distribution of the parameters is approximated
 as importance samples of weight 
\begin_inset Formula $L_{i}\times w_{i}$
\end_inset

 at the dead point locations, and can be resampled into a set of points
 with equal weights, for posterior analyses similar to those with Markov
 Chains.
 More details on the convergence and error estimates can be found in 
\begin_inset CommandInset citation
LatexCommand citet
key "skilling2009nested"

\end_inset

.
\end_layout

\begin_layout Standard
Efficient general solutions exist for drawing a new point above a likelihood
 threshold in low dimensions (
\begin_inset Formula $n_{{\rm dim}}<20$
\end_inset

).
 The idea is to draw only in the neighbourhood of the live points, which
 already fulfill the likelihood threshold.
 The best-known algorithm in astrophysics and cosmology is 
\shape smallcaps
multinest
\shape default
 
\begin_inset CommandInset citation
LatexCommand citep
key "Feroz2009"

\end_inset

.
 There, the contours traced out by the points are clustered into ellipses,
 and new points drawn from the ellipses.
 To avoid accidentally cutting away too much of the parameter space, the
 tightest-fitting ellipses are enlarged by an empirical (problem-specific)
 factor.
 Another algorithm is 
\shape smallcaps
RadFriends
\shape default
 
\begin_inset CommandInset citation
LatexCommand citep
key "buchner2014statistical"

\end_inset

, which defines the neighbourhood as all points within a radius 
\begin_inset Formula $r$
\end_inset

 of an existing live point.
 By leaving out randomly a portion of the live points, and determining their
 distance to the other live points, the largest nearest-neighbour radius
 
\begin_inset Formula $r$
\end_inset

 is determined.
 The worst-case analysis through bootstrapping cross-validation over multiple
 rounds makes 
\shape smallcaps
RadFriends
\shape default
 robust, independent of contour shapes and free of tuning parameters.
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:IllustrationNS"

\end_inset

 illustrates the generated regions.
 
\shape smallcaps
RadFriends
\shape default
 is efficient if one chooses a standardised euclidean metric (i.e.
 normalise by the standard deviation of the live points along each axis).
 I use this algorithm in this work, although any other method can be substituted.
\end_layout

\begin_layout Subsection
Simplified description of the idea
\end_layout

\begin_layout Standard
Consider two independent nested sampling runs on different data sets, but
 initialised to the same random number generator.
 Initially points are generated from across the entire parameter space,
 typically giving bad fits.
 If the data sets are somewhat similar, this phase of zooming to the relevant
 parameter space will be the same for the two runs.
 Importantly, while the exact likelihood value will be different for the
 same point, the ordering of the points will be similar.
 In other words, for both, the worst-fitting point to be removed is the
 same.
 The next key insight is that new points can efficiently be drawn from a
 contour which is the union of the likelihood contours from both runs.
 Ideally, the point can be accepted by both runs, keeping the runs similar
 (black points in Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:illustration-pointsharing"

\end_inset

).
 The speed-up of the algorithm comes from the fact that when a point is
 shared, the (slow) predicting model has to be only evaluated once.
 The model prediction is then compared against the data to produce a likelihood
 for each data set, an operation which I presume to be fast (e.g.
 simply computing 
\begin_inset Formula $-\sum(x_{i}-m_{i})^{2}/2\sigma_{i}^{2}$
\end_inset

 where 
\begin_inset Formula $m_{i}$
\end_inset

/
\begin_inset Formula $x_{i}$
\end_inset

/
\begin_inset Formula $\sigma_{i}$
\end_inset

 are the predictions/measurements/errors in data space respectively).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename plotjointcontour_1.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:illustration-pointsharing"

\end_inset

The analysis of two similar data sets yields at the same iteration similar
 likelihood contours (cyan, magenta dotted lines).
 In the presented algorithm a large fraction of live points are shared across
 data sets (black points), which reduces the number of model evaluations.
 The differences (cyan, magenta points) requiring additional draws.
\end_layout

\end_inset


\end_layout

\end_inset

What if the point can be accepted by only one run? It can not simply be
 rejected, otherwise the uniform sampling property of nested sampling is
 broken.
 Instead, accepted points are stored in queues, one for each run/data set.
 Once both runs have a non-empty queue, the first accepted point is removed
 from each queue and replaces the dead point of each data set.
 If a point was only accepted by one run, but the following point is accepted
 by both runs, this second point becomes a live point immediately for one
 run, but can later also become a live point for the other run (if it suffices
 the likelihood threshold at that later iteration).
 This technique allows sustained sharing of points, decreasing the number
 of unique live points and increasing the speed-up.
\end_layout

\begin_layout Standard
At a later point in the algorithm, the contours may significantly diverge
 and not share any live points.
 This is because the data sets do not have the same best-fit parameters.
 Then, nested sampling runs can continue as in the classic case, and there
 is no speed-up, falling back to a linear scaling.
 This happens earlier the more different the data sets are.
 The run is longer for data sets with high signal-to-noise, making the algorithm
 efficient when most observations are near the detection limit, as is typically
 the case in surveys (a consequence of powerlaw distributions).
\end_layout

\begin_layout Subsection
Complete description of the algorithm
\end_layout

\begin_layout Standard
I now describe the algorithm for simultaneously analysing 
\begin_inset Formula $N$
\end_inset

 data sets, where 
\begin_inset Formula $N$
\end_inset

 is a large number.
\end_layout

\begin_layout Subsubsection
Likelihood Function
\end_layout

\begin_layout Standard
The likelihood function receives a single parameter vector, and information
 which data sets to consider.
 It calls the physical model with the parameter vector to compute into a
 prediction into data space.
 The physical model may perform complex and slow numerical computations/simulati
ons at this point.
 
\end_layout

\begin_layout Standard
Finally the prediction is compared to the individual data sets to produce
 a likelihood for each considered data set.
 The likelihood at this point can be Gaussian (see above), Poisson (comparing
 the predicted counts to observed counts), a red noise Gaussian process,
 or any other probability distribution as appropriate for the instrument.
 In any case, this computation must be fast compared to producing the model
 predictions to receive any performance gains.
\end_layout

\begin_layout Subsubsection
Nested Sampling Integrator
\end_layout

\begin_layout Standard
The integrator essentially deals with each run individually as in standard
 nested sampling, keeping track of the volume at the current iteration,
 and storing the live points and their weights for each data set individually.
 It calls the constrained sampler (see below), which holds the live points,
 to receive the next dead point (for all data sets simultaneously).
 The integrator must also test for convergence, and advance further only
 those runs which have not yet converged.
 Here I use the standard criterion that the nested sampling error is 
\begin_inset Formula $\delta Z<0.5$
\end_inset

 (from last equation in 
\begin_inset CommandInset citation
LatexCommand citealp
key "skilling2009nested"

\end_inset

).
 Once all runs have terminated, corresponding to each data set the integral
 estimates 
\begin_inset Formula $Z$
\end_inset

 and posterior samples are returned, giving the user the same output as
 e.g.
 a 
\shape smallcaps
multinest
\shape default
 analysis.
\end_layout

\begin_layout Subsubsection
Constrained Sampler
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename shelves.dia
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:queues"

\end_inset

To replace the least likely live point, new points are sampled and placed
 in queues if they have a high enough likelihood.
 Once every data set has a non-empty queue, the lowest points are pushed
 out and stored as dead points by the integrator.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The sampler initially draws 
\begin_inset Formula $N_{{\rm live}}$
\end_inset

 live points and stores likelihood array of size (
\begin_inset Formula $N\times N_{{\rm live}}$
\end_inset

).
 Sequential IDs are assigned to live points and the mapping between live
 point IDs and data sets (
\begin_inset Formula $N\times N_{{\rm live}}$
\end_inset

 point indices) stored.
 The integrator informs the sampler when it should remove the lowest likelihood
 point and replace it.
 The integrator also informs the sampler when some data sets have finished
 and can be discarded from further consideration, in which case the sampler
 works as if they had never participated.
\end_layout

\begin_layout Standard
The main task of the constrained sampler is to do joint draws under likelihood
 constraint 
\begin_inset Formula $L>L_{{\rm min}}$
\end_inset

 to replace the lowest likelihood point of each of the 
\begin_inset Formula $d$
\end_inset

 data sets.
 For this, 
\begin_inset Formula $d$
\end_inset

 initially empty queues are introduced (see Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:queues"

\end_inset

).
 First, it is attempted to draw from the joint contour, i.e.
 letting 
\shape smallcaps
RadFriends
\shape default
 define a region based on the all live points and drawing from this region
 (
\emph on
superset draw
\emph default
).
 Some points will be accepted and the corresponding queues are filled.
 If this fails to fill all queues after several (10) attempts, a 
\emph on
focussed draw
\emph default
 is done.
 In that case, only the data sets with empty queues are considered, the
 region is constructed from live points associated with them, and the likelihood
 only evaluated for these data sets.
 For example, in the illustration of Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:queues"

\end_inset

, only Data Set 3 would be considered.
 Once all queues have at least one entry, nested sampling can advance: For
 each data set, the first queue entry is removed and replaces the dead live
 point.
 The dead live points are returned to the integrator.
\end_layout

\begin_layout Standard
Storing queue entries is only useful if they can replace live points later.
 That means, to be accepted into the end of the queue at position 
\begin_inset Formula $j$
\end_inset

, it must have a likelihood higher than 
\begin_inset Formula $j$
\end_inset

 points from the runs live points and previous entries of the queue.
 In other words, the first entry must merely beat a single existing live
 point, the second entry must beat both a live point and either another
 live point or the first queue entry (which will become a live point).
\end_layout

\begin_layout Subsubsection
Data Set Clustering
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename graph.dia
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:graph"

\end_inset

Association of live point objects with data sets.
 In this illustration, some live points are shared between the group of
 Data Set 1-3; these form a connected subgraph.
 Data Set 4 has separate live points and can be treated independently.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
It can occur that the live points between two groups of data sets are not
 shared any more.
 For instance, one may have a dichotomy between broad and narrow line objects,
 and the contours identify some of the data sets in the former, some in
 the latter regime.
 Then it is not useful to consider all live points when defining the region,
 because it introduces unnecessary multimodality.
 Instead, subsets can be identified which share live points (see Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:graph"

\end_inset

), and these subsets can be processed independently.
 Algorithms for identifying connected subsets of graphs are well-known.
 The necessary graph can be constructed with nodes corresponding to the
 data sets, nodes corresponding to the live points, and connecting the graph
 according to the current live point statuses.
 This produces some overhead especially for large 
\begin_inset Formula $N$
\end_inset

, but it can be done lazily when needed.
 A few simple checks can rule out subsets: If there are fewer unique live
 points across all data sets than 
\begin_inset Formula $2\times N_{{\rm live}}$
\end_inset

, some must be shared and there are no subsets.
 We can also track any live point which has been accepted by all data sets.
 Such a 
\emph on
superpoint
\emph default
, until one data set removes it as a dead point, guarantees that there are
 no disconnected subsets.
 With the described analysis of divergence of contours, the algorithm effectivel
y performs a clustering in similarity between data sets.
 This can speed up region draws, as it eliminates space between clusters,
 and improves superset draws because unrelated data sets which can not benefit
 are not pulled in.
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Results"

\end_inset


\end_layout

\begin_layout Standard
A simple example problem illustrates the use and scaling of the algorithm.
 We consider a spectroscopic survey with 
\begin_inset Formula $N$
\end_inset

 fibers, which collected spectra in the 
\begin_inset Formula $400-800{\rm nm}$
\end_inset

 wavelength range.
 We look for a Gaussian line at 
\begin_inset Formula $440{\rm nm}$
\end_inset

 rest frame with line width of 
\begin_inset Formula $400\,{\rm km/s}$
\end_inset

, i.e.
 spanning multiple spectral bins.
 We generate random data sets from 
\begin_inset Formula $N=1$
\end_inset

 to 
\begin_inset Formula $N=10^{5}$
\end_inset

 and analyse them separately to understand the scaling of the algorithm.
 The simulated sample of 
\begin_inset Formula $N$
\end_inset

 objects has a redshift distribution extending out to 
\begin_inset Formula $z\sim1$
\end_inset

.
 The amplitudes vary with a normal distribution around 
\begin_inset Formula $20$
\end_inset

 with standard deviation 
\begin_inset Formula $20$
\end_inset

, but at least 
\begin_inset Formula $10$
\end_inset

, in units of the Gaussian noise standard deviation.
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spectra"

\end_inset

 shows some high and low signal-to-noise examples of the simulated data
 set.
\end_layout

\begin_layout Standard
For the analysis we define our model parameter space as amplitude, width
 and location of a single Gaussian line, with log-uniform/log-uniform/uniform
 priors from 
\begin_inset Formula $10^{0-2}$
\end_inset

, 
\begin_inset Formula $10^{2-4}{\rm km/s}$
\end_inset

 and 
\begin_inset Formula $400-800{\rm nm}$
\end_inset

.
 The Gaussian line is our 
\begin_inset Quotes eld
\end_inset

slow-to-compute
\begin_inset Quotes erd
\end_inset

 physical model.
 The likelihood function is a simple product of Gaussian errors.
 A more elaborate example would include physical modelling of multiple emission
 lines with Doppler broadening, outflows and red noise, without necessitating
 modification of the presented algorithm.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /mnt/data/daten/PostDoc/research/stats/massivedatans/genfaint.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:spectra"

\end_inset

Simulated noisy data.
 The location, width and amplitude of a single line is sought in Gaussian
 noise for the illustrative problem.
 Here we show four spectra, with the true line locations indicated by triangles.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:cost"

\end_inset

 shows the number of model evaluations necessary for analysing 
\begin_inset Formula $N$
\end_inset

 data sets.
 The algorithm scales much better than the baseline linear scaling.
 For instance, it takes only a few times more model evaluations to analyse
 
\begin_inset Formula $10,000$
\end_inset

 observations than 
\begin_inset Formula $20$
\end_inset

 observations, a speedup of over two orders of magnitude.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /mnt/data/daten/PostDoc/research/stats/massivedatans/plotscaling.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:cost"

\end_inset

Number of model evaluations of the algorithm.
 A naive approach of independent analyses would have a linear scaling (black
 line).
 The algorithm (red points) scales substantially better, even below 
\begin_inset Formula $O(\sqrt{N})$
\end_inset

 in the considered problem.
 The scatter comes from generating new random data sets every time which
 can randomly differ in their signal-to-noise ratio.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /mnt/data/daten/PostDoc/research/stats/massivedatans/plotposterior.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:post"

\end_inset

Parameter posterior distributions.
 The four examples from Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spectra"

\end_inset

 are shown in the same colours.
 Gray is used for the larger errorbars and black otherwise.
 The yellow and magenta data set have been well-detected and characterized,
 whereas the pink and magenta are too uncertain to claim any detection.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /mnt/data/daten/PostDoc/research/stats/massivedatans/plotposteriorz.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:postz"

\end_inset

Redshift distributions for objects where the line was clearly constrained
 (blue) compared to the input distribution (black).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We can now plot the posterior distributions of the found line locations.
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:post"

\end_inset

 demonstrates the wide variety of uncertainties.
 The spectra of Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spectra"

\end_inset

 are shown in the same colours.
 For many, the line could be identified and characterised with small uncertainti
es (yellow, cyan, black), for others, the method remains unsure (pink, magenta,
 gray).
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:postz"

\end_inset

 shows that the input redshift distribution is correctly recovered.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /mnt/data/daten/PostDoc/research/stats/massivedatans/plotevidences.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:postZ"

\end_inset

Bayes factors between the single-line model and a no-line model.
 The black histogram are Bayes factors from analysing the test data set.
 The red histogram shows the same, but for a Monte Carlo data set without
 any signal.
 Because the latter has no values above 
\begin_inset Formula $B\gtrapprox2$
\end_inset

, for the black histogram at 
\begin_inset Formula $B\gtrapprox2$
\end_inset

, a line can be claimed detected with a low false positive fraction.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
After parameter estimation we can consider model comparison: Is the line
 significantly detected? For this, we consider the Bayes factor, 
\begin_inset Formula $B=Z_{1}/Z_{0}$
\end_inset

, where 
\begin_inset Formula $Z_{1}$
\end_inset

 is the integral computed by nested sampling under the single-line model,
 and 
\begin_inset Formula $Z_{0}$
\end_inset

 is the same for the null hypothesis (no line).
 The latter can be analytically computed as 
\begin_inset Formula $\ln Z_{0}=-\left(\sum(x_{i}/\sigma_{i})^{2}+\ln2\pi\sigma_{i}^{2}\right)/2$
\end_inset

.
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:postZ"

\end_inset

 shows in black the derived Bayes factors (truncated at 
\begin_inset Formula $10^{4}$
\end_inset

).
 To define a lower threshold for significant detections, we Monte Carlo
 simulate a dataset with 
\begin_inset Formula $N=1000$
\end_inset

 spectra without signal, and derive 
\begin_inset Formula $Z_{1}$
\end_inset

 values.
 This can be done rapidly with the presented algorithm.
 The red histogram in Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:postZ"

\end_inset

 show the resulting Bayes factors.
 The highest B-value in this signal-free data set is 
\begin_inset Formula $B\approx2$
\end_inset

.
 Therefore, in the 
\begin_inset Quotes eld
\end_inset

real
\begin_inset Quotes erd
\end_inset

 data, those with a Bayes factor 
\begin_inset Formula $B>2$
\end_inset

 can be securely claimed to have a line, with a small false detections fraction
 (
\begin_inset Formula $p<0.001$
\end_inset

).
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
A scalable algorithm for analysing massive data sets with arbitrary physical
 models and complex, inhomogeneous noise properties was presented.
 The algorithm brings parameter estimation with uncertainties, classification
 of objects and distinction between physical processes to the Big Data regime.
\end_layout

\begin_layout Standard
The key insight is to take advantage of a property specific to nested sampling:
 The contours at a given iteration are sub-volumes which can look similar
 across similar data sets, and it is permitted to draw new points from the
 union of contours.
 These joint draws reduces the number of unique model evaluations drastically,
 in particular at the beginning of the nested sampling run.
 The same approach can not be followed with Markov Chain proposals: There,
 the following proposal depends on the current points, and different acceptances
 prohibit a later joint proposal.
 The management of many nests suggests 
\emph on
Bird Colony Algorithm
\emph default
 as a name.
\end_layout

\begin_layout Standard
The algorithm has some overhead related to the management of live points,
 in particularly to determine the unique set of live points across a dynamically
 selected subgroup of data sets.
 The memory usage also grows if big data sets have to be held in the same
 machine.
 If only chunks of 
\begin_inset Formula $N$
\end_inset

 are managable, the analyses can be split into such sizes and analysed in
 parallel across multiple machines.
 In that case, one can take advantage of the scaling of the algorithm until
 
\begin_inset Formula $N$
\end_inset

.
\end_layout

\begin_layout Standard
The algorithm can be applied immediately to any existing large data sets,
 such as spectra from the Sloan Digital Sky Survey 
\begin_inset CommandInset citation
LatexCommand citep
key "Eisenstein2011SDSS"

\end_inset

 which inspired the example presented here.
 Low signal-to-noise data or exhaustive searches for lines with multiple
 solutions are addressed in the algorithm.
 As another example, eROSITA 
\begin_inset CommandInset citation
LatexCommand citep
key "Predehl2014eRosita"

\end_inset

 requires the source classification and characterisation of 3 million point
 sources 
\begin_inset CommandInset citation
LatexCommand citep
key "Kolodzig2012"

\end_inset

.
 The desire to use existing physical models, the complex detector response
 and non-Gaussianity of the errors makes standard machine learning approaches
 difficult to apply.
 Aside from surveys with large data sets, individual integral-field-unit
 observations, where each pixel contains a spectrum, can be analysed with
 the algorithm.
 There, often many pixels will share lines with similar velocities, the
 optimal speed-up case for the algorithm.
 Even in the analysis of single objects, one might want to do Monte Carlo
 simulations of a null hypothesis model to test the correctness of decisions
 to select a more complex model, e.g.
 based on Bayes Factors.
 The presented method allows speedy analysis of such Monte Carlo simulation
 data sets.
\end_layout

\begin_layout Section*
Acknowledgements
\end_layout

\begin_layout Standard
We acknowledge support from the CONICYT-Chile grants Basal-CATA PFB-06/2007,
 FONDECYT Postdoctorados 3160439 and the Ministry of Economy, Development,
 and Tourism's Millennium Science Initiative through grant IC120009, awarded
 to The Millennium Institute of Astrophysics, MAS.
 This research was supported by the DFG cluster of excellence 
\begin_inset Quotes eld
\end_inset

Origin and Structure of the Universe
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "/mnt/data/daten/PostDoc/literature/agn,/mnt/data/daten/PostDoc/literature/grb,/mnt/data/daten/PostDoc/literature/sim"
options "mnras"

\end_inset


\end_layout

\end_body
\end_document
