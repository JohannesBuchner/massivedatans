#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass svjour3
\options twocolumn
\use_default_options true
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "unicode=true"
\papersize a4paper
\use_geometry false
\use_package amsmath 2
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\rightmargin 5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newcommand{
\backslash
mnras}{MNRAS}
\end_layout

\begin_layout Plain Layout


\backslash
newcommand{
\backslash
apjl}{ApJ}
\end_layout

\begin_layout Plain Layout


\backslash
newcommand{
\backslash
aj}{AJ}
\end_layout

\begin_layout Plain Layout


\backslash
newcommand{
\backslash
mdash}{-}
\end_layout

\begin_layout Plain Layout


\backslash
journalname{Statistics and Computing}
\end_layout

\begin_layout Plain Layout


\backslash
institute{
\end_layout

\begin_layout Plain Layout

Millenium Institute of Astrophysics, Vicu
\backslash
~{n}a.
 MacKenna 4860, 7820436 Macul, Santiago, Chile
\end_layout

\begin_layout Plain Layout


\backslash

\backslash

\end_layout

\begin_layout Plain Layout

Pontificia Universidad Católica de Chile, Instituto de Astrofísica, Casilla
 306, Santiago 22, Chile
\end_layout

\begin_layout Plain Layout


\backslash

\backslash

\end_layout

\begin_layout Plain Layout

Excellence Cluster Universe, Boltzmannstr.
 2, D-85748, Garching, Germany
\end_layout

\begin_layout Plain Layout


\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
email{johannes.buchner.acad@gmx.com}
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

%@arxiver{plotscaling.pdf,plotjointcontour_1.pdf,plotcontour_5.pdf}
\end_layout

\begin_layout Plain Layout


\backslash
titlerunning{Big Data vs.
 complex physical models}
\end_layout

\end_inset


\end_layout

\begin_layout Title
Big Data vs.
 complex physical models: a scalable inference algorithm 
\end_layout

\begin_layout Author
J.
 Buchner
\end_layout

\begin_layout Date
Received: date / Accepted: date
\end_layout

\begin_layout Date
Accepted XXX.
 Received YYY; in original form ZZZ
\end_layout

\begin_layout Abstract
The data torrent unleashed by current and upcoming astronomical surveys
 demands scalable analysis methods.
 Machine Learning approaches scale well.
 However, separating the instrument measurement from the physical effects
 of interest, dealing with variable errors, and deriving parameter uncertainties
 is usually an after-thought.
 Classic forward-folding analyses with Markov Chain Monte Carlo or Nested
 Sampling enable parameter estimation and model comparison, even for complex
 and slow-to-evaluate physical models.
 However, these approaches require independent runs for each data set, implying
 an unfeasible number of model evaluations in the Big Data regime.
 Here we present a new algorithm, collaborative nested sampling, for deriving
 parameter probability distributions for each observation.
 Importantly, in our method the number of physical model evaluations scales
 sub-linearly with the number of data sets, and we make no assumptions about
 homogeneous errors, Gaussianity, the form of the model or heterogeneity/complet
eness of the observations.
 Collaborative nested sampling has immediate application in speeding up
 analyses of large surveys, integral-field-unit observations, and Monte
 Carlo simulations.
\end_layout

\begin_layout Keywords
Nested sampling, Big Data, Bayesian inference
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Big Data has arrived in astronomy 
\begin_inset CommandInset citation
LatexCommand cite
key "Feigelson2012,Zhang2015a,Mickaelian2016,Kremer2017"

\end_inset

.
 In the previous century it was common to analyse a few dozen objects in
 detail.
 For instance, one would use Markov Chain Monte Carlo to forward fold a
 physical model and constrain its parameters.
 This would be repeated for each member of the sample.
 However, current and upcoming instruments provide a wealth of data (
\begin_inset Formula $\sim$
\end_inset

millions of independent sources) where it becomes computationally difficult
 to follow the same approach, even though it is embarrassingly parallel.
 Currently, much effort is put into studying and applying machine learning
 algorithms such as (deep learning) neural networks or random forests for
 the analysis of massive datasets.
 This can work well if the measurement errors are homogeneous, but typically
 these methods make it difficult to insert existing physical knowledge into
 the analysis, to deal with variable errors and missing data points, and
 generally to separate the instrument measurement process from the physical
 effects of interest.
 Furthermore, we would like to derive probability density distributions
 of physical parameters for each object, and do model comparison between
 physical effects/sources classes.
\end_layout

\begin_layout Standard
In this work I show how nested sampling can be used to analyse 
\begin_inset Formula $N$
\end_inset

 data sets simultaneously.
 The key insight is that nested sampling allows effective sharing of evaluation
 points across data sets, requiring much fewer model evaluations than if
 the 
\begin_inset Formula $N$
\end_inset

 data sets were analysed individually.
 I only assume that the model can be split into two components: a slow-to-evalua
te physical model which performs a prediction into observable space, and
 a fast-to-compute comparison to the individual data sets (e.g.
 the likelihood of a probability distribution).
 Otherwise, the user is free to chose arbitrary physical models and likelihoods.
 In §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Results"

\end_inset

, I present as an example the line fitting of a hypothetical many-object
 spectroscopic survey.
 A more advanced example could include broadened H/O/C line emissions from
 various ionisation states under red noise errors, without modification
 of our algorithm.
 
\end_layout

\begin_layout Section
Methodology
\end_layout

\begin_layout Subsection
Introduction to Classic Nested Sampling
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename plotcontour_1.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename plotcontour_3.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename plotcontour_5.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename plotcontour_6.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:IllustrationNS"

\end_inset

Illustration of nested sampling.
 At a given iteration of the nested sampling algorithm, the live points
 (black) trace out the current likelihood constraint, a region (dashed)
 which is unknown.
 The 
\shape smallcaps
RadFriends
\shape default
 algorithm conservatively reconstructs the region (orange) by including
 everything within a certain, adaptively chosen radius of the current live
 points.
 Between iterations, the likelihood contour is elevated, making the sampled
 volume smaller and smaller.
\end_layout

\end_inset


\end_layout

\end_inset

Nested sampling 
\begin_inset CommandInset citation
LatexCommand citep
key "Skilling2004"

\end_inset

 is a global parameter space exploration algorithm, which zooms in from
 the entire volume towards the best-fit models by steadily increasing the
 likelihood threshold.
 In the process it produces parameter posterior probability distributions
 and computes the integral over the parameter space.
 Assume that the parameter space is a 
\begin_inset Formula $k$
\end_inset

-dimensional cube.
 A number of live points 
\begin_inset Formula $N_{{\rm live}}$
\end_inset

 are randomly
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
In general, following the prior.
 For most problems one can assume uniform sampling with appropriate stretching
 of the parameter space under the inverse cumulative of the prior distributions.
\end_layout

\end_inset

 placed in the parameter space.
 Their likelihood is evaluated.
 Each point represents 
\begin_inset Formula $1/N_{{\rm live}}$
\end_inset

 of the entire volume.
 The live point with the lowest likelihood 
\begin_inset Formula $L_{{\rm min}}$
\end_inset

 is then removed, implying the removal of space with likelihood below 
\begin_inset Formula $L_{{\rm min}}$
\end_inset

 and shrinkage of the volume to 
\begin_inset Formula $1-\exp\left(-1/N_{{\rm live}}\right)$
\end_inset

, on average.
 A new random live point is drawn, with the requirement that its likelihood
 must be above 
\begin_inset Formula $L_{{\rm min}}$
\end_inset

.
 This replacement procedure is iterated, shrinking the volume exponentially.
 Each removed (
\begin_inset Quotes eld
\end_inset

dead
\begin_inset Quotes erd
\end_inset

) point and its likelihood 
\begin_inset Formula $L_{i}$
\end_inset

 is stored.
 The integral over the parameter space can then be approximated by 
\begin_inset Formula $Z=\sum_{i}L_{i}\times w_{i}$
\end_inset

, where 
\begin_inset Formula $w_{i}$
\end_inset

 is the removed volume at the iteration.
 At a late stage in the algorithm the volume probed is tiny and the likelihood
 
\begin_inset Formula $L_{i}$
\end_inset

 increase is negligible, so that the weights 
\begin_inset Formula $L_{i}\times w_{i}$
\end_inset

 of the remaining live points becomes small.
 Then the iterative procedure can be stopped (the algorithm converged).
 The posterior probability distribution of the parameters is approximated
 as importance samples of weight 
\begin_inset Formula $L_{i}\times w_{i}$
\end_inset

 at the dead point locations, and can be resampled into a set of points
 with equal weights, for posterior analyses similar to those with Markov
 Chains.
 More details on the convergence and error estimates can be found in 
\begin_inset CommandInset citation
LatexCommand citet
key "skilling2009nested"

\end_inset

.
\end_layout

\begin_layout Standard
Efficient general solutions exist for drawing a new point above a likelihood
 threshold in low dimensions (
\begin_inset Formula $n_{{\rm dim}}<20$
\end_inset

).
 The idea is to draw only in the neighbourhood of the current live points,
 which already fulfill the likelihood threshold.
 The best-known algorithm in astrophysics and cosmology is 
\shape smallcaps
multinest
\shape default
 
\begin_inset CommandInset citation
LatexCommand citep
key "Shaw2007,Feroz2009"

\end_inset

.
 There, the contours traced out by the points are clustered into ellipses,
 and new points drawn from the ellipses.
 To avoid accidentally cutting away too much of the parameter space, the
 tightest-fitting ellipses are enlarged by an empirical (problem-specific)
 factor.
 Another algorithm is 
\shape smallcaps
RadFriends
\shape default
 
\begin_inset CommandInset citation
LatexCommand citep
key "buchner2014statistical"

\end_inset

, which defines the neighbourhood as all points within a radius 
\begin_inset Formula $r$
\end_inset

 of an existing live point.
 By leaving out randomly a portion of the live points, and determining their
 distance to the remaining live points, the largest nearest-neighbour radius
 
\begin_inset Formula $r$
\end_inset

 is determined.
 The worst-case analysis through bootstrapping cross-validation over multiple
 rounds makes 
\shape smallcaps
RadFriends
\shape default
 robust, independent of contour shapes and free of tuning parameters.
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:IllustrationNS"

\end_inset

 illustrates the generated regions.
 
\shape smallcaps
RadFriends
\shape default
 is efficient if one chooses a standardised euclidean metric (i.e.
 normalise by the standard deviation of the live points along each axis).
 I use this algorithm in this work, although any other method can be substituted.
\end_layout

\begin_layout Subsection
Simplified description of the idea
\end_layout

\begin_layout Standard
Consider two independent nested sampling runs on different data sets, but
 initialised to the same random number generator state.
 Initially points are generated from across the entire parameter space,
 typically giving bad fits.
 If the data sets are somewhat similar, this phase of zooming to the relevant
 parameter space will be the same for the two runs.
 Importantly, while the exact likelihood value will be different for the
 same point, the ordering of the points will be similar.
 In other words, for both, the worst-fitting point to be removed is the
 same.
 The next key insight is that new points can be drawn efficiently from a
 contour which is the union of the likelihood contours from both runs.
 Ideally, the point can be accepted by both runs, keeping the runs similar
 (black points in Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:illustration-pointsharing"

\end_inset

).
 When a point is shared, the (slow) predicting model has to be only evaluated
 once, speeding up the run.
 The model prediction is then compared against the data to produce a likelihood
 for each data set, an operation which I presume to be fast (e.g.
 simply computing 
\begin_inset Formula 
\[
{\cal L}_{j}=-\sum_{i}(x_{ij}-m_{i})^{2}/2\sigma_{ij}^{2}
\]

\end_inset

 where 
\begin_inset Formula $m_{i}$
\end_inset

, 
\begin_inset Formula $x_{ij}$
\end_inset

 and 
\begin_inset Formula $\sigma_{ij}$
\end_inset

 are the predictions, measurements and errors in data space respectively
 for data set 
\begin_inset Formula $j$
\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename plotjointcontour_1.pdf
	width 100col%
	BoundingBox 45bp 0bp 369bp 160bp
	clip

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:illustration-pointsharing"

\end_inset

The analysis of two similar data sets yields at the same iteration similar
 likelihood contours (the two dotted ellipses).
 In the presented algorithm a large fraction of live points are shared across
 data sets (black points), which reduces the number of model evaluations.
 The differences (cyan crosses and magenta pluses) requiring additional
 draws.
\end_layout

\end_inset


\end_layout

\end_inset

What if the point can be accepted by only one run? It cannot simply be rejected,
 otherwise the uniform sampling property of nested sampling is broken.
 Instead, accepted points are stored in queues, one for each run/data set.
 Once both runs have a non-empty queue, the first accepted point is removed
 from each queue and replaces the dead point of each data set.
 Joint sampling also helps even if a point is not useful right away.
 If a point was only accepted by one run, but the following point is accepted
 by both runs, the latter becomes a live point immediately for one run,
 but can later also become a live point for the other run (if it suffices
 the likelihood threshold at that later iteration).
 This technique allows sustained sharing of points, decreasing the number
 of unique live points and increasing the speed-up.
\end_layout

\begin_layout Standard
At a later point in the algorithm, the contours may significantly diverge
 and not share any live points.
 This is because the best-fit parameters of data sets will differ.
 Then, nested sampling runs can continue as in the classic case, without
 speed-up, falling back to a linear scaling.
 This happens earlier, the more different the data sets are.
 The run is longer for data sets with high signal-to-noise, making the algorithm
 most efficient when most observations are near the detection limit.
 This is typically the case in surveys (a consequence of powerlaw distributions).
\end_layout

\begin_layout Subsection
Complete description of the algorithm
\begin_inset Foot
status open

\begin_layout Plain Layout
A proof-of-concept reference implementation is available at 
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/JohannesBuchner/massivedatans/"
target "https://github.com/JohannesBuchner/massivedatans/"

\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
I now describe the algorithm for simultaneously analysing 
\begin_inset Formula $N$
\end_inset

 data sets, where 
\begin_inset Formula $N$
\end_inset

 is a large number.
 The algorithm components are the nested sampling integrator, the constrained
 sampler and the likelihood function, as in classic nested sampling, except
 that work on 
\begin_inset Formula $N$
\end_inset

 data sets.
 The constrained sampler behaves substantially different in our algorithm.
 
\end_layout

\begin_layout Subsubsection
Likelihood Function
\end_layout

\begin_layout Standard
The likelihood function receives a single parameter vector, and information
 which data sets to consider.
 It calls the physical model with the parameter vector to compute into a
 prediction into data space.
 The physical model may perform complex and slow numerical computations/simulati
ons at this point.
 
\end_layout

\begin_layout Standard
Finally the prediction is compared with the individual data sets to produce
 a likelihood for each considered data set.
 The likelihood at this point can be Gaussian (see above), Poisson (comparing
 the predicted counts to observed counts), a red noise Gaussian process,
 or any other probability distribution as appropriate for the instrument.
 In any case, this computation must be fast compared with producing the
 model predictions to receive any performance gains.
\end_layout

\begin_layout Subsubsection
Nested Sampling Integrator
\end_layout

\begin_layout Standard
The integrator essentially deals with each run individually as in standard
 nested sampling, keeping track of the volume at the current iteration,
 and storing the live points and their weights for each data set individually.
 It calls the constrained sampler (see below), which holds the live points,
 to receive the next dead point (for all data sets simultaneously).
 The integrator must also test for convergence, and advance further only
 those runs which have not yet converged.
 Here I use the standard criterion that the nested sampling error is 
\begin_inset Formula $\delta Z<0.5$
\end_inset

 (from last equation in 
\begin_inset CommandInset citation
LatexCommand citealp
key "skilling2009nested"

\end_inset

).
 Once all runs have terminated, corresponding to each data set the integral
 estimates 
\begin_inset Formula $Z$
\end_inset

 and posterior samples are returned, giving the user the same output as
 e.g.
 a 
\shape smallcaps
multinest
\shape default
 analysis.
\end_layout

\begin_layout Subsubsection
Constrained Sampler
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename shelves.dia
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:queues"

\end_inset

To replace the least likely live point, new points are sampled and placed
 in queues if they have a high enough likelihood.
 Once every data set has a non-empty queue, the lowest points are pushed
 out and stored as dead points by the integrator.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The sampler initially draws 
\begin_inset Formula $N_{{\rm live}}$
\end_inset

 live points and stores their likelihoods in an array of size 
\begin_inset Formula $N\times N_{{\rm live}}$
\end_inset

.
 Sequential IDs are assigned to live points and the mapping between live
 point IDs and data sets (
\begin_inset Formula $N\times N_{{\rm live}}$
\end_inset

 indices) is stored.
 The integrator informs the sampler when it should remove the lowest likelihood
 point and replace it.
 The integrator also informs the sampler when some data sets have finished
 and can be discarded from further consideration, in which case the sampler
 works as if they had never participated.
\end_layout

\begin_layout Standard
The main task of the constrained sampler is to do joint draws under likelihood
 constraint 
\begin_inset Formula $L>L_{{\rm min}}$
\end_inset

 to replace the lowest likelihood point in each of the 
\begin_inset Formula $d$
\end_inset

 data sets.
 For this, 
\begin_inset Formula $d$
\end_inset

 initially empty queues are introduced (see Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:queues"

\end_inset

).
 First, it is attempted to draw from the joint contour over all data sets
 (
\emph on
superset draw
\emph default
), i.e.
 letting 
\shape smallcaps
RadFriends
\shape default
 define a region based on the all unique live points.
 From this region a point is drawn which has 
\begin_inset Formula $L>L_{{\rm min}}$
\end_inset

 for at least one data set.
 Some will accept and the corresponding queues are filled.
 If this fails to fill all queues after several (e.g.
 10) attempts, a 
\emph on
focussed draw
\emph default
 is done.
 In that case, only the data sets with empty queues are considered, the
 region is constructed from their live points, and the likelihood only evaluated
 for these data sets.
 For example, in the illustration of Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:queues"

\end_inset

, only Data Set 3 would be considered.
 Once all queues have at least one entry, nested sampling can advance: For
 each data set, the first queue entry is removed and replaces the dead live
 point.
 In Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:queues"

\end_inset

 this is illustrated by the queues pushing out the lowest live points.
 These dead points are returned to the integrator.
\end_layout

\begin_layout Standard
Storing queue entries is only useful if they can replace live points in
 the upcoming iterations.
 Playing nested sampling forward this implies that to be accepted into the
 end of the queue at position 
\begin_inset Formula $j$
\end_inset

, it must have a likelihood higher than 
\begin_inset Formula $j$
\end_inset

 points from the runs live points and previous entries of the queue.
 In other words, the first entry must merely beat a single existing live
 point, the second entry must beat both a live point and either another
 live point or the first queue entry (which will become a live point in
 the next iteration).
\end_layout

\begin_layout Subsubsection
Data Set Clustering
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename graph.dia
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:graph"

\end_inset

Association of live point objects with data sets.
 In this illustration, some live points are shared between the group of
 Data Set 1-3; these form a connected subgraph.
 Data Set 4 has separate live points and can be treated independently.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
It can occur that between two groups of data sets the live points are not
 shared any more, i.e.
 the live point sets are disjoint.
 For example, one may have a dichotomy between broad and narrow line objects,
 and the contours identify some of the data sets in the former, some in
 the latter class.
 Then it is not useful to consider all live points when defining the region,
 because it introduces unnecessary multi-modality.
 Instead, subsets can be identified which share live points (see Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:graph"

\end_inset

), and these subsets can be processed independently.
 Algorithms for identifying connected subsets of graphs are well-known.
 The necessary graph can be constructed with nodes corresponding to the
 data sets, nodes corresponding to the live points, and connecting the graph
 according to the current live point statuses.
 This introduces some computational overhead, especially for large 
\begin_inset Formula $N$
\end_inset

 and 
\begin_inset Formula $N_{{\rm live}}$
\end_inset

.
 However, one can lazily defer graph construction and maintenance until
 they are needed.
 A few simple checks can trivially rule out disjoint subsets: If there are
 fewer unique live points across all data sets than 
\begin_inset Formula $2\times N_{{\rm live}}$
\end_inset

, some must be shared and there are no disjoint subsets.
 We can also track any live point which has been accepted by all data sets.
 Any such a 
\emph on
superpoint
\emph default
, until one data set removes it as a dead point, guarantees that there are
 no disconnected subsets.
 
\end_layout

\begin_layout Standard
The described analysis of divergence of contours effectively clusters data
 sets by similarity.
 Interestingly, the similarity is not defined in data space, nor in parameter
 space, but only through the constraints in parameter space.
 This is important because clustering in data space can be non-trivial for
 data with varying errors and completeness, and clustering in parameter
 space would scale poorly with model dimensionality because it is metric
 dependent.
 Instead, the likelihood ordering that makes nested sampling unique is taken
 advantage of.
 The clustering improves the efficiency of region draws, as it eliminates
 space between clusters, and improves super-set draws because unrelated
 data sets, which cannot benefit, are not pulled in.
 
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Results"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename /mnt/data/daten/PostDoc/research/stats/massivedatans/hornsrun/genhorns.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:spectra"

\end_inset

Simulated noisy data.
 The location, width and amplitude of a single line is sought in Gaussian
 noise for the illustrative problem.
 Here we show four spectra, with the true line locations indicated by triangles.
 The cyan data set shows a random fluctuation at 
\begin_inset Formula $700{\rm nm}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset

A simple example problem illustrates the use and scaling of the algorithm.
 We consider a spectroscopic survey which collected 
\begin_inset Formula $N$
\end_inset

 spectra in the 
\begin_inset Formula $400-800{\rm nm}$
\end_inset

 wavelength range.
 We look for a Gaussian line at 
\begin_inset Formula $654{\rm nm}$
\end_inset

 rest frame (but randomly shifted) with standard deviation of 
\begin_inset Formula $0.5{\rm nm}$
\end_inset

.
 The amplitudes vary with a powerlaw distribution with index 
\begin_inset Formula $3$
\end_inset

, and a minimum value of 
\begin_inset Formula $2$
\end_inset

 in units of the Gaussian noise standard deviation.
 We generate a large random data set and analyse the first 
\begin_inset Formula $N$
\end_inset

 data sets simultaneously to understand the scaling of the algorithm, with
 
\begin_inset Formula $N=1$
\end_inset

 to 
\begin_inset Formula $N=10^{4}$
\end_inset

.
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spectra"

\end_inset

 presents some high and low signal-to-noise examples of the simulated data
 set.
\end_layout

\begin_layout Standard
The parameter space of the analysis has three dimensions: The amplitude,
 width and location of a single Gaussian line, with log-uniform/log-uniform/unif
orm priors from 
\begin_inset Formula $10^{0-2}$
\end_inset

, 
\begin_inset Formula $0.15-15{\rm nm}$
\end_inset

 and 
\begin_inset Formula $600-1000{\rm nm}$
\end_inset

 respectively.
 The Gaussian line is our 
\begin_inset Quotes eld
\end_inset

slow-to-compute
\begin_inset Quotes erd
\end_inset

 physical model.
 The likelihood function is a simple product of Gaussian errors.
 A more elaborate example would include physical modelling of an ionised
 outflow emitting multiple lines with Doppler broadening and red detector
 noise, without necessitating any modification of the presented algorithm.
\end_layout

\begin_layout Standard
Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:cost"

\end_inset

 shows the number of model evaluations necessary for analysing 
\begin_inset Formula $N$
\end_inset

 data sets.
 The algorithm scales much better than the baseline linear scaling, i.e.
 analysing the data sets individually one-by-one.
 For instance, it takes only twenty times more model evaluations to analyse
 
\begin_inset Formula $1000$
\end_inset

 observations than a single observations, a 
\begin_inset Formula $50$
\end_inset

-fold speedup.
 Indeed, the algorithm scales in this problem much better than the naive
 linear cost 
\begin_inset Formula $O(N)$
\end_inset

 of parallel analyses.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /mnt/data/daten/PostDoc/research/stats/massivedatans/hornsrun/plotscaling.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:cost"

\end_inset

Number of model evaluations of the algorithm.
 A naive approach of independent analyses would have a linear scaling (black
 line).
 The algorithm (red points) scales substantially better, similar to 
\begin_inset Formula $O(\sqrt{N})$
\end_inset

 in the considered problem.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /mnt/data/daten/PostDoc/research/stats/massivedatans/hornsrun/plotposterior.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:post"

\end_inset

Parameter posterior distributions.
 The four examples from Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spectra"

\end_inset

 are shown in the same colours.
 Gray is used for the larger errorbars and black otherwise.
 The line in the pink and yellow data sets have been well-detected and character
ized, while the magenta has larger uncertainties.
 The cyan constraints cover two solutions (see Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spectra"

\end_inset

).
 Error bars are centred at the median of the posteriors with the line lengths
 reflecting the 1-sigma equivalent quantiles.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /mnt/data/daten/PostDoc/research/stats/massivedatans/hornsrun/plotposteriorz.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:postz"

\end_inset

Line location distribution for objects where the line was well-constrained
 (blue) compared to the input distribution (black).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We can now plot the posterior distributions of the found line locations.
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:post"

\end_inset

 demonstrates the wide variety of uncertainties.
 The spectra of Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spectra"

\end_inset

 are shown in the same colours.
 For many, the line could be identified and characterised with small uncertainti
es (cyan, black), for others, the method remains unsure (pink, yellow, magenta,
 gray).
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:postz"

\end_inset

 shows that the input redshift distribution is correctly recovered.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /mnt/data/daten/PostDoc/research/stats/massivedatans/hornsrun/plotevidences.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:postZ"

\end_inset

Bayes factors between the single-line model and a no-line model.
 The black histogram shows Bayes factors from analysing the test data set.
 The red histogram shows Bayes factors from a Monte Carlo data set without
 any signal.
 Because the latter has very few values above 
\begin_inset Formula $B\gtrapprox10$
\end_inset

, for the black histogram at 
\begin_inset Formula $B\gtrapprox10$
\end_inset

, a line can be claimed detected with a low false positive fraction.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
After parameter estimation we can consider model comparison: is the line
 significantly detected? For this, lets consider the Bayes factor, 
\begin_inset Formula $B=Z_{1}/Z_{0}$
\end_inset

, where 
\begin_inset Formula $Z_{1}$
\end_inset

 is the integral computed by nested sampling under the single-line model,
 and 
\begin_inset Formula $Z_{0}$
\end_inset

 is the same for the null hypothesis (no line).
 The latter can be analytically computed as 
\begin_inset Formula $\ln Z_{0}=-\frac{1}{2}\left[\sum(x_{i}/\sigma_{i})^{2}+\ln2\pi\sigma_{i}^{2}\right]$
\end_inset

.
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:postZ"

\end_inset

 shows in black the derived Bayes factors (truncated at 
\begin_inset Formula $10^{4}$
\end_inset

).
 To define a lower threshold for significant detections, we Monte Carlo
 simulate a dataset with 
\begin_inset Formula $N=10,000$
\end_inset

 spectra without signal, and derive 
\begin_inset Formula $Z_{1}$
\end_inset

 values.
 This can be done rapidly with the presented algorithm.
 The red histogram in Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:postZ"

\end_inset

 shows the resulting Bayes factors.
 The 
\begin_inset Formula $99.9\%$
\end_inset

 quantile of 
\begin_inset Formula $B$
\end_inset

-values in this signal-free data set is 
\begin_inset Formula $B\approx10$
\end_inset

.
 Therefore, in the 
\begin_inset Quotes eld
\end_inset

real
\begin_inset Quotes erd
\end_inset

 data, those with a Bayes factor 
\begin_inset Formula $B>10$
\end_inset

 can be securely claimed to have a line, with a small false detections fraction
 (
\begin_inset Formula $p<0.001$
\end_inset

).
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
A scalable algorithm for analysing massive data sets with arbitrarily detailed
 physical models and complex, inhomogeneous noise properties was presented.
 The algorithm brings to the Big Data regime parameter estimation with uncertain
ties, classification of objects and distinction between physical processes.
 
\end_layout

\begin_layout Standard
The key insight in this work is to take advantage of a property specific
 to nested sampling: The contours at a given iteration are sub-volumes which
 can look similar across similar data sets, and it is permitted to draw
 new points from the union of contours
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
As in classic nested sampling, the volume shrinkage estimates are valid
 on average.
 Multiple runs can test whether this leads to additional scatter in the
 integral estimate.
 In practice, single runs already give correct uncertainties for many problems.
\end_layout

\end_inset

.
 These joint draws reduce drastically the number of unique model evaluations,
 in particular at the beginning of the nested sampling run.
 The same approach cannot be followed with Markov Chain proposals: There,
 the following proposal depends on the current points, and different acceptances
 prohibit a later joint proposal.
 The joint sampling suggests 
\emph on
Collaborative Nested Sampling
\emph default
 as the name of the algorithm.
\end_layout

\begin_layout Standard
The algorithm has some overhead related to the management of live points,
 in particularly to determine the unique set of live points across a dynamically
 selected subgroup of data sets.
 The memory usage also grows if big data sets have to be held in the same
 machine.
 If only chunks of 
\begin_inset Formula $N$
\end_inset

 are managable, the analyses can be split into such sizes and analysed in
 parallel across multiple machines.
 In that case, one can take advantage of the scaling of the algorithm until
 
\begin_inset Formula $N$
\end_inset

.
 
\end_layout

\begin_layout Standard

\series bold
Applications: 
\series default
The algorithm can be applied immediately to any existing large data sets,
 such as spectra from the Sloan Digital Sky Survey 
\begin_inset CommandInset citation
LatexCommand citep
key "Eisenstein2011SDSS"

\end_inset

 which inspired the example presented here.
 Low signal-to-noise data or exhaustive searches for lines with multiple
 solutions are addressed in the algorithm.
 Aside from surveys with large data sets, individual integral-field-unit
 observations, where each pixel contains a spectrum, can be analysed with
 the algorithm, permitting also the fitting of complex ionisation or radiative
 transfer models.
 There the optimal speed-up case for the algorithm is satisfied, because
 often many pixels will share lines with similar positions and widths.
 Compared to existing approaches, nested sampling naturally allows model
 comparisons (e.g.
 detection of additional lines) and multiple fit solutions.
\end_layout

\begin_layout Standard
As another example, 
\emph on
eROSITA
\emph default
 
\begin_inset CommandInset citation
LatexCommand citep
key "Predehl2014eRosita"

\end_inset

 requires the source classification and characterisation of 3 million point
 sources in its all-sky survey 
\begin_inset CommandInset citation
LatexCommand citep
key "Kolodzig2012"

\end_inset

.
 The desire to use existing physical models, the complex detector response
 and non-Gaussianity of count data make standard machine learning approaches
 difficult to apply.
 Furthermore, standard fitting techniques can fail to correctly converge
 and individual visual inspection in the Big Data regime is impractical.
\end_layout

\begin_layout Standard
Even in the analysis of single objects the presented algorithm can help.
 One might test the correctness of selecting a more complex model, e.g.
 based on Bayes Factors, as in the toy example presented.
 Large Monte Carlo simulations of a null hypothesis model can be quickly
 analysed with the presented method, with a model evaluation cost that is
 essentially independent of the number of generated data sets, i.e.
 comparable to the single-object analysis.
\end_layout

\begin_layout Section*
Acknowledgements
\end_layout

\begin_layout Standard
I thank Surangkhana Rukdee and Frederik Beaujean for reading the manuscript.
\end_layout

\begin_layout Standard
I acknowledge support from the CONICYT-Chile grants Basal-CATA PFB-06/2007,
 FONDECYT Postdoctorados 3160439 and the Ministry of Economy, Development,
 and Tourism's Millennium Science Initiative through grant IC120009, awarded
 to The Millennium Institute of Astrophysics, MAS.
 This research was supported by the DFG cluster of excellence 
\begin_inset Quotes eld
\end_inset

Origin and Structure of the Universe
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "/mnt/data/daten/PostDoc/literature/agn"
options "spmpsci"

\end_inset


\end_layout

\end_body
\end_document
