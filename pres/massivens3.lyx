#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass svjour3
\options twocolumn
\use_default_options true
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "unicode=true"
\papersize a4paper
\use_geometry false
\use_package amsmath 2
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\rightmargin 5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newcommand{
\backslash
mnras}{MNRAS}
\end_layout

\begin_layout Plain Layout


\backslash
newcommand{
\backslash
apjl}{ApJ}
\end_layout

\begin_layout Plain Layout


\backslash
newcommand{
\backslash
apj}{ApJL}
\end_layout

\begin_layout Plain Layout


\backslash
newcommand{
\backslash
aj}{AJ}
\end_layout

\begin_layout Plain Layout


\backslash
newcommand{
\backslash
procspie}{SPIE}
\end_layout

\begin_layout Plain Layout


\backslash
newcommand{
\backslash
mdash}{-}
\end_layout

\begin_layout Plain Layout


\backslash
journalname{Publications of the Astronomical Society of the Pacific}
\end_layout

\begin_layout Plain Layout


\backslash
institute{
\end_layout

\begin_layout Plain Layout

Millenium Institute of Astrophysics, Vicu
\backslash
~{n}a.
 MacKenna 4860, 7820436 Macul, Santiago, Chile
\end_layout

\begin_layout Plain Layout


\backslash

\backslash

\end_layout

\begin_layout Plain Layout

Pontificia Universidad Católica de Chile, Instituto de Astrofísica, Casilla
 306, Santiago 22, Chile
\end_layout

\begin_layout Plain Layout


\backslash

\backslash

\end_layout

\begin_layout Plain Layout

Excellence Cluster Universe, Boltzmannstr.
 2, D-85748, Garching, Germany
\end_layout

\begin_layout Plain Layout


\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
email{johannes.buchner.acad@gmx.com}
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

%@arxiver{outimg_noise0.2.pdf,plotscaling.pdf,plotcontour_5.pdf}
\end_layout

\end_inset


\end_layout

\begin_layout Title
Collaborative Nested Sampling: Big Data vs.
 complex physical models
\end_layout

\begin_layout Author
Johannes Buchner
\end_layout

\begin_layout Date
.
\end_layout

\begin_layout Abstract
The data torrent unleashed by current and upcoming astronomical surveys
 demands scalable analysis methods.
 Machine learning approaches scale well.
 However, separating the instrument measurement from the physical effects
 of interest, dealing with variable errors, and deriving parameter uncertainties
 is usually an after-thought.
 Classic forward-folding analyses with Markov Chain Monte Carlo or Nested
 Sampling enable parameter estimation and model comparison, even for complex
 and slow-to-evaluate physical models.
 However, these approaches require independent runs for each data set, implying
 an unfeasible number of model evaluations in the Big Data regime.
 Here I present a new algorithm, collaborative nested sampling, for deriving
 parameter probability distributions for each observation.
 Importantly, the number of physical model evaluations scales sub-linearly
 with the number of data sets, and no assumptions about homogeneous errors,
 Gaussianity, the form of the model or heterogeneity/completeness of the
 observations need to be made.
 Collaborative nested sampling has immediate application in speeding up
 analyses of large surveys, integral-field-unit observations, and Monte
 Carlo simulations.
\end_layout

\begin_layout Keywords
Nested sampling, Big Data, Bayesian inference
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Big Data has arrived in astronomy 
\begin_inset CommandInset citation
LatexCommand cite
key "Feigelson2012,Zhang2015a,Mickaelian2016,Kremer2017"
literal "true"

\end_inset

.
 In the previous century it was common to analyse a few dozen objects in
 detail.
 For instance, one would use Markov Chain Monte Carlo to forward fold a
 physical model and constrain its parameters.
 This would be repeated for each member of the sample.
 However, current and upcoming instruments provide a wealth of data (
\begin_inset Formula $\sim$
\end_inset

 millions of independent sources) where it becomes computationally difficult
 to follow the same approach, even though it is embarrassingly parallel.
 Currently, much effort is put into studying and applying machine learning
 algorithms such as (deep learning) neural networks or random forests for
 the analysis of massive datasets.
 This can work well if the measurement errors are homogeneous, but typically
 these methods make it difficult to insert existing physical knowledge into
 the analysis, to deal with variable errors and missing data points, and
 generally to separate the instrument measurement process from the physical
 effects of interest.
 Furthermore, we would like to derive probability density distributions
 of physical parameters for each object, and do model comparison between
 physical effects/sources classes.
\end_layout

\begin_layout Standard
In this work I show how nested sampling can be used to analyse 
\begin_inset Formula $N$
\end_inset

 data sets simultaneously.
 The key insight is that nested sampling allows effective sharing of evaluation
 points across data sets, requiring much fewer model evaluations than if
 the 
\begin_inset Formula $N$
\end_inset

 data sets were analysed individually.
 I only assume that the model can be split into two components: a slow-to-evalua
te physical model which performs a prediction into observable space, and
 a fast-to-compute comparison to the individual data sets (e.g.
 the likelihood of a probability distribution).
 Otherwise, the user is free to chose arbitrary physical models and likelihoods.
 §
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Results"

\end_inset

 presents a line fitting of a hypothetical many-object spectroscopic survey
 as a toy example; §
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ifuresults"
plural "false"
caps "false"
noprefix "false"

\end_inset

 constrains the properties of stellar populations in a real imaging-spectroscopy
 observation.
\end_layout

\begin_layout Section
Methodology
\end_layout

\begin_layout Subsection
Introduction to Classic Nested Sampling
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename plotcontour_1.pdf
	width 100col%
	BoundingBox 0bp 10bp 369bp 160bp
	clip

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename plotcontour_3.pdf
	width 100col%
	BoundingBox 0bp 30bp 369bp 140bp
	clip

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename plotcontour_5.pdf
	width 100col%
	BoundingBox 0bp 40bp 369bp 130bp
	clip

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename plotcontour_6.pdf
	width 100col%
	BoundingBox 0bp 50bp 369bp 130bp
	clip

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:IllustrationNS"

\end_inset

Illustration of nested sampling.
 At a given iteration of the nested sampling algorithm, the live points
 (black) trace out the current likelihood constraint, a region (dashed)
 which is unknown.
 The 
\shape smallcaps
RadFriends
\shape default
 algorithm conservatively reconstructs the region (orange) by including
 everything within a certain, adaptively chosen radius of the current live
 points.
 Between iterations, the likelihood contour is elevated, making the sampled
 volume smaller and smaller.
\end_layout

\end_inset


\end_layout

\end_inset

Nested sampling 
\begin_inset CommandInset citation
LatexCommand citep
key "Skilling2004"
literal "true"

\end_inset

 is a global parameter space exploration algorithm, which zooms in from
 the entire volume towards the best-fit models by steadily increasing the
 likelihood threshold.
 In the process it produces parameter posterior probability distributions
 and computes the integral over the parameter space.
 Assume that the parameter space is a 
\begin_inset Formula $k$
\end_inset

-dimensional cube.
 A number of live points 
\begin_inset Formula $N_{{\rm live}}$
\end_inset

 are randomly
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
In general, following the prior.
 For most problems one can assume uniform sampling with appropriate stretching
 of the parameter space under the inverse cumulative of the prior distributions.
\end_layout

\end_inset

 placed in the parameter space.
 Their likelihood is evaluated.
 Each point represents 
\begin_inset Formula $1/N_{{\rm live}}$
\end_inset

 of the entire volume.
 The live point with the lowest likelihood 
\begin_inset Formula $L_{{\rm min}}$
\end_inset

 is then removed, implying the removal of space with likelihood below 
\begin_inset Formula $L_{{\rm min}}$
\end_inset

 and shrinkage of the volume to 
\begin_inset Formula $1-\exp\left(-1/N_{{\rm live}}\right)$
\end_inset

, on average.
 A new random live point is drawn, with the requirement that its likelihood
 must be above 
\begin_inset Formula $L_{{\rm min}}$
\end_inset

.
 This replacement procedure is iterated, shrinking the volume exponentially.
 Each removed (
\begin_inset Quotes eld
\end_inset

dead
\begin_inset Quotes erd
\end_inset

) point and its likelihood 
\begin_inset Formula $L_{i}$
\end_inset

 is stored.
 The integral over the parameter space can then be approximated by 
\begin_inset Formula $Z=\sum_{i}L_{i}\times w_{i}$
\end_inset

, where 
\begin_inset Formula $w_{i}$
\end_inset

 is the removed volume at the iteration.
 At a late stage in the algorithm the volume probed is tiny and the likelihood
 
\begin_inset Formula $L_{i}$
\end_inset

 increase is negligible, so that the weights 
\begin_inset Formula $L_{i}\times w_{i}$
\end_inset

 of the remaining live points becomes small.
 Then the iterative procedure can be stopped (the algorithm converged).
 The posterior probability distribution of the parameters is approximated
 as importance samples of weight 
\begin_inset Formula $L_{i}\times w_{i}$
\end_inset

 at the dead point locations, and can be resampled into a set of points
 with equal weights, for posterior analyses similar to those with Markov
 Chains.
 More details on the convergence and error estimates can be found in 
\begin_inset CommandInset citation
LatexCommand citet
key "skilling2009nested"
literal "true"

\end_inset

.
\end_layout

\begin_layout Standard
Efficient general solutions exist for drawing a new point above a likelihood
 threshold in low dimensions (
\begin_inset Formula $n_{{\rm dim}}<20$
\end_inset

).
 The idea is to draw only in the neighbourhood of the current live points,
 which already fulfill the likelihood threshold.
 The best-known algorithm in astrophysics and cosmology is 
\shape smallcaps
multinest
\shape default
 
\begin_inset CommandInset citation
LatexCommand citep
key "Shaw2007,Feroz2009"
literal "true"

\end_inset

.
 There, the contours traced out by the points are clustered into ellipses,
 and new points drawn from the ellipses.
 To avoid accidentally cutting away too much of the parameter space, the
 tightest-fitting ellipses are enlarged by an empirical (problem-specific)
 factor.
 Another algorithm is 
\shape smallcaps
RadFriends
\shape default
 
\begin_inset CommandInset citation
LatexCommand citep
key "buchner2014statistical"
literal "true"

\end_inset

, which defines the neighbourhood as all points within a radius 
\begin_inset Formula $r$
\end_inset

 of an existing live point.
 By leaving out randomly a portion of the live points, and determining their
 distance to the remaining live points, the largest nearest-neighbour radius
 
\begin_inset Formula $r$
\end_inset

 is determined.
 The worst-case analysis through bootstrapping cross-validation over multiple
 rounds makes 
\shape smallcaps
RadFriends
\shape default
 robust, independent of contour shapes and free of tuning parameters.
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:IllustrationNS"

\end_inset

 illustrates the generated regions.
 
\shape smallcaps
RadFriends
\shape default
 is efficient if one chooses a standardised euclidean metric (i.e.
 normalise by the standard deviation of the live points along each axis).
 I use this algorithm below, however any other method can be substituted.
\end_layout

\begin_layout Subsection
Simplified description of the idea
\end_layout

\begin_layout Standard
Consider two independent nested sampling runs on different data sets, but
 initialised to the same random number generator state.
 Initially points are generated from across the entire parameter space,
 typically giving bad fits.
 If the data sets are somewhat similar, the phase of zooming to the relevant
 parameter space will be the same for the two runs.
 Importantly, while the exact likelihood value will be different for the
 same point, the ordering of the points will be similar.
 In other words, for both, the worst-fitting point to be removed is the
 same.
 The next key insight is that new points can be drawn efficiently from a
 contour which is the union of the likelihood contours from both runs.
 Ideally, the point can be accepted by both runs, keeping the runs similar
 (black points in Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:illustration-pointsharing"

\end_inset

).
 When a point is shared, the (slow) predicting model has to be only evaluated
 once, speeding up the run.
 The model prediction is then compared against the data to produce a likelihood
 for each data set, an operation which I presume to be fast, e.g., when computing
 
\begin_inset Formula 
\begin{equation}
{\cal L}_{j}=-\sum_{i}(x_{ij}-m_{i})^{2}/(2\sigma_{ij}^{2})\label{eq:chisquare}
\end{equation}

\end_inset

 where 
\begin_inset Formula $m_{i}$
\end_inset

, 
\begin_inset Formula $x_{ij}$
\end_inset

 and 
\begin_inset Formula $\sigma_{ij}$
\end_inset

 are the predictions, measurements and errors in data space respectively
 for data set 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename plotjointcontour_1.pdf
	width 100col%
	BoundingBox 45bp 0bp 369bp 160bp
	clip

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:illustration-pointsharing"

\end_inset

The analysis of two similar data sets yields at the same iteration similar
 likelihood contours (the two dotted ellipses).
 In the presented algorithm a large fraction of live points are shared across
 data sets (black points), which reduces the number of model evaluations.
 The differences (cyan crosses and magenta pluses) requiring additional
 draws.
\end_layout

\end_inset


\end_layout

\end_inset

What if the point can be accepted by only one run? It cannot simply be rejected
 or accepted in both, otherwise the uniform sampling property of nested
 sampling is broken.
 Instead, accepted points are stored in queues, one for each run/data set.
 Once both runs have a non-empty queue, the first accepted point is removed
 from each queue and replaces the dead point of each data set.
 Joint sampling also helps even if a point is not useful right away.
 If a point was only accepted by run A, but the following point is accepted
 by both runs, the second point becomes a live point immediately for run
 B, but can later also become a live point for run A (if it suffices the
 likelihood threshold at that later iteration).
 This technique allows sustained sharing of points, decreasing the number
 of unique live points and increasing the speed-up.
\end_layout

\begin_layout Standard
At a later point in the algorithm, the contours may significantly diverge
 and not share any live points.
 This is because the best-fit parameters of data sets will differ.
 Then, nested sampling runs can continue as in the classic case, without
 speed-up, falling back to a linear scaling.
 This happens earlier, the more different the data sets are.
 The run is longer for data sets with high signal-to-noise, making the algorithm
 most efficient when most observations are near the detection limit.
 This is typically the case in surveys as a consequence of powerlaw distribution
s.
\end_layout

\begin_layout Subsection
Collaborative nested sampling
\end_layout

\begin_layout Standard
I now describe the collaborative nested sampling algorithm.
 A proof-of-concept reference implementation is available at 
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/JohannesBuchner/massivedatans/"
target "https://github.com/JohannesBuchner/massivedatans/"
literal "false"

\end_inset

.
 The algorithm components are the nested sampling integrator, the constrained
 sampler and the likelihood function, as in classic nested sampling, except
 that works on 
\begin_inset Formula $N$
\end_inset

 data sets simultaneously, with 
\begin_inset Formula $N$
\end_inset

 a large number.
 The constrained sampler behaves substantially different in this algorithm.
\end_layout

\begin_layout Subsubsection
Likelihood Function
\end_layout

\begin_layout Standard
The likelihood function receives a single parameter vector, and information
 which data sets to consider.
 It calls the physical model with the parameter vector to compute into a
 prediction into data space.
 The physical model may perform complex and slow numerical computations/simulati
ons at this point.
 Finally the prediction is compared with the individual data sets to produce
 a likelihood for each considered data set.
 The likelihood at this point can be Gaussian (see above), Poisson, a red
 noise process, or any other probability distribution appropriate for the
 instrument.
 In any case, this computation must be fast compared with producing the
 model predictions to receive any performance gains.
\end_layout

\begin_layout Subsubsection
Nested Sampling Integrator
\end_layout

\begin_layout Standard
The integrator deals with each run individually just as in standard nested
 sampling.
 It keeps track of the remaining volume at the current iteration, and storing
 the live points and their weights for each data set individually.
 It calls the constrained sampler (see below), which holds the live points,
 to receive the next dead point (for all data sets simultaneously).
 The integrator must also test for convergence, and advance further only
 those runs which have not yet converged.
 Here I use the standard criterion that the nested sampling error is 
\begin_inset Formula $\delta Z<0.5$
\end_inset

 (from last equation in 
\begin_inset CommandInset citation
LatexCommand citealp
key "skilling2009nested"
literal "true"

\end_inset

).
 Once all runs have terminated, corresponding to each data set the integral
 estimates 
\begin_inset Formula $Z$
\end_inset

 and posterior samples are returned, giving the user the same output as
 e.g., a 
\shape smallcaps
multinest
\shape default
 analysis.
\end_layout

\begin_layout Subsubsection
Constrained Sampler
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename shelves.dia
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:queues"

\end_inset

To replace the least likely live point, new points are sampled and placed
 in queues if they have a high enough likelihood.
 Once every data set has a non-empty queue, the lowest points are pushed
 out and stored as dead points by the integrator.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The sampler initially draws 
\begin_inset Formula $N_{{\rm live}}$
\end_inset

 live points and stores their likelihoods in an array of size 
\begin_inset Formula $N\times N_{{\rm live}}$
\end_inset

.
 Sequential IDs are assigned to live points and the mapping between live
 point IDs and data sets (
\begin_inset Formula $N\times N_{{\rm live}}$
\end_inset

 indices) is stored.
 The integrator informs the sampler when it should remove the lowest likelihood
 point and replace it.
 The integrator also informs the sampler when some data sets have finished
 and can be discarded from further consideration, in which case the sampler
 works as if they had never participated.
\end_layout

\begin_layout Standard
The main task of the constrained sampler is to do joint draws under likelihood
 constraint 
\begin_inset Formula $L>L_{{\rm min}}$
\end_inset

 to replace the lowest likelihood point in each of the 
\begin_inset Formula $d$
\end_inset

 data sets.
 For this, 
\begin_inset Formula $d$
\end_inset

 initially empty queues are introduced (see Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:queues"

\end_inset

).
 First, it is attempted to draw from the joint contour over all data sets
 (
\emph on
superset draw
\emph default
), i.e.
 letting 
\shape smallcaps
RadFriends
\shape default
 define a region based on the all unique live points.
 From this region a point is drawn which has 
\begin_inset Formula $L>L_{{\rm min}}$
\end_inset

 for at least one data set.
 Some will accept and the corresponding queues are filled.
 If this fails to fill all queues after several (e.g.
 10) attempts, a 
\emph on
focussed draw
\emph default
 is done.
 In that case, only the data sets with empty queues are considered, the
 region is constructed from their live points, and the likelihood only evaluated
 for these data sets.
 For example, in the illustration of Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:queues"

\end_inset

, only Data Set 3 would be considered.
 Once all queues have at least one entry, nested sampling can advance: For
 each data set, the first queue entry is removed and replaces the dead live
 point.
 In Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:queues"

\end_inset

 this is illustrated by the queues pushing out the lowest live points.
 These dead points are returned to the integrator.
\end_layout

\begin_layout Standard
Storing queue entries is only useful if they can replace live points in
 the upcoming iterations.
 Playing nested sampling forward this implies that to be accepted into the
 end of the queue at position 
\begin_inset Formula $j$
\end_inset

, it must have a likelihood higher than 
\begin_inset Formula $j$
\end_inset

 points from the runs live points and previous entries of the queue.
 In other words, the first entry must merely beat a single existing live
 point, the second entry must beat both a live point and either another
 live point or the first queue entry (which will become a live point in
 the next iteration).
\end_layout

\begin_layout Subsubsection
Data Set Clustering
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename graph.dia
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:graph"

\end_inset

Association of live point objects with data sets.
 In this illustration, some live points are shared between the group of
 Data Set 1-3; these form a connected subgraph.
 Data Set 4 has separate live points and can be treated independently.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
It can occur that between two groups of data sets the live points are not
 shared any more, i.e.
 the live point sets are disjoint.
 For example, one may have a dichotomy between broad and narrow line objects,
 and the contours identify some of the data sets in the former, some in
 the latter class.
 Then it is not useful to consider all live points when defining the region,
 because it introduces unnecessary multi-modality.
 Instead, subsets can be identified which share live points (see Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:graph"

\end_inset

), and these subsets can be processed independently.
 Algorithms for identifying connected subsets of graphs are well-known.
 The necessary graph can be constructed with nodes corresponding to the
 data sets, nodes corresponding to the live points, and connecting the graph
 according to the current live point statuses.
 This introduces some computational overhead, especially for large 
\begin_inset Formula $N$
\end_inset

 and 
\begin_inset Formula $N_{{\rm live}}$
\end_inset

.
 However, one can lazily defer graph construction and maintenance until
 they are needed.
 A few simple checks can trivially rule out disjoint subsets: If there are
 fewer unique live points across all data sets than 
\begin_inset Formula $2\times N_{{\rm live}}$
\end_inset

, some must be shared and there are no disjoint subsets.
 We can also track any live point which has been accepted by all data sets.
 Any such a 
\emph on
superpoint
\emph default
, until one data set removes it as a dead point, guarantees that there are
 no disconnected subsets.
\end_layout

\begin_layout Standard
The described analysis of divergence of contours effectively clusters data
 sets by similarity.
 Interestingly, the similarity is not defined in data space, nor in parameter
 space, but only through the constraints in parameter space.
 This is important because clustering in data space can be non-trivial for
 data with varying errors and completeness, and clustering in parameter
 space would scale poorly with model dimensionality.
 Instead, the likelihood ordering that makes nested sampling unique is taken
 advantage of.
 The clustering improves the efficiency of region draws, as it eliminates
 space between clusters.
 It also improves super-set draws as unrelated data sets, which cannot benefit,
 are not pulled in.
\end_layout

\begin_layout Section
Toy Application: Single-line fitting
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Results"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /mnt/data/daten/PostDoc/research/stats/massivedatans/hornsrun/genhorns.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:spectra"

\end_inset

Simulated noisy data.
 The location, width and amplitude of a single line is sought in Gaussian
 noise for the illustrative problem.
 The true line locations of the four spectra are indicated by triangles.
 The cyan data set shows a random fluctuation at 
\begin_inset Formula $700{\rm nm}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset

A simple toy example problem illustrates the use and scaling of the algorithm.
 Lets consider a spectroscopic survey which collected 
\begin_inset Formula $N$
\end_inset

 spectra in the 
\begin_inset Formula $400-800{\rm nm}$
\end_inset

 wavelength range.
 We look for a Gaussian line at 
\begin_inset Formula $654{\rm nm}$
\end_inset

 rest frame (but randomly shifted) with standard deviation of 
\begin_inset Formula $0.5{\rm nm}$
\end_inset

.
 The amplitudes vary with a powerlaw distribution with index 
\begin_inset Formula $3$
\end_inset

, with a signal-to-noise ratio of at least two.
 I generate a large random data set and analyse the first 
\begin_inset Formula $N$
\end_inset

 data sets simultaneously to understand the scaling of the algorithm, with
 
\begin_inset Formula $N=1$
\end_inset

 to 
\begin_inset Formula $N=10^{4}$
\end_inset

.
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spectra"

\end_inset

 presents some high and low signal-to-noise examples of the simulated data
 set.
\end_layout

\begin_layout Standard
The parameter space of the analysis has three dimensions: The amplitude,
 width and location of a single Gaussian line, with log-uniform/log-uniform/unif
orm priors from 
\begin_inset Formula $10^{0-2}$
\end_inset

, 
\begin_inset Formula $0.15-15{\rm nm}$
\end_inset

 and 
\begin_inset Formula $600-1000{\rm nm}$
\end_inset

 respectively.
 The Gaussian line is our 
\begin_inset Quotes eld
\end_inset

slow-to-compute
\begin_inset Quotes erd
\end_inset

 physical model.
 The likelihood function is as in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:chisquare"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 A more elaborate example would include physical modelling of an ionised
 outflow emitting multiple lines with Doppler broadening and red detector
 noise, without necessitating any modification of the presented algorithm.
\end_layout

\begin_layout Standard
Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:cost"

\end_inset

 shows the number of model evaluations necessary for analysing 
\begin_inset Formula $N$
\end_inset

 data sets.
 The black line shows the baseline linear scaling 
\begin_inset Formula $O(N)$
\end_inset

, i.e.
 analysing the data sets individually one-by-one.
 The algorithm scales much better, close to 
\begin_inset Formula $O(\sqrt{N})$
\end_inset

.
 For instance, it takes only 
\begin_inset Formula $20$
\end_inset

 times more model evaluations to analyse 
\begin_inset Formula $1000$
\end_inset

 observations than a single observations, a 
\begin_inset Formula $50$
\end_inset

-fold speedup.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /mnt/data/daten/PostDoc/research/stats/massivedatans/hornsrun/plotscaling.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:cost"

\end_inset

Number of model evaluations of the algorithm.
 A naive approach of independent analyses would have a linear scaling (black
 line).
 The algorithm (red points) scales substantially better, similar to 
\begin_inset Formula $O(\sqrt{N})$
\end_inset

 in the considered problem.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /mnt/data/daten/PostDoc/research/stats/massivedatans/hornsrun/plotposterior.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:post"

\end_inset

Parameter posterior distributions.
 The four examples from Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spectra"

\end_inset

 are shown in the same colours.
 The line in the pink and yellow data sets have been well-detected and character
ized, while the magenta ine has larger uncertainties.
 The cyan constraints cover two solutions (see Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spectra"

\end_inset

).
 Error bars are centred at the median of the posteriors with the line lengths
 reflecting the 1-sigma equivalent quantiles.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /mnt/data/daten/PostDoc/research/stats/massivedatans/hornsrun/plotposteriorz.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:postz"

\end_inset

Line location distribution for objects where the line was well-constrained
 (blue) compared to the input distribution (black).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We can now plot the posterior distributions of the found line locations.
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:post"

\end_inset

 demonstrates the wide variety of uncertainties.
 The spectra of Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:spectra"

\end_inset

 are shown in the same colours.
 For many, the line could be identified and characterised with small uncertainti
es (cyan, black), for others, the method remains unsure (pink, yellow, magenta,
 gray).
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:postz"

\end_inset

 shows that the input redshift distribution is correctly recovered.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /mnt/data/daten/PostDoc/research/stats/massivedatans/hornsrun/plotevidences.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:postZ"

\end_inset

Bayes factors between the single-line model and a no-line model.
 The black histogram shows Bayes factors from analysing the test data set.
 The red histogram shows Bayes factors from a noise-only data.
 Because the latter has very few values above 
\begin_inset Formula $B\gtrapprox10$
\end_inset

, a line can be claimed detected beyond that threshold with a low false
 positive fraction.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
After parameter estimation we can consider model comparison: is the line
 significantly detected? For this, lets consider the Bayes factor, 
\begin_inset Formula $B=Z_{1}/Z_{0}$
\end_inset

, where 
\begin_inset Formula $Z_{1}$
\end_inset

 is the integral computed by nested sampling under the single-line model,
 and 
\begin_inset Formula $Z_{0}$
\end_inset

 is the same for the null hypothesis (no line).
 The latter can be analytically computed as 
\begin_inset Formula $\ln Z_{0}=-\frac{1}{2}\left[\sum(x_{i}/\sigma_{i})^{2}+\ln2\pi\sigma_{i}^{2}\right]$
\end_inset

.
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:postZ"

\end_inset

 shows in black the derived Bayes factors.
 To define a lower threshold for significant detections, I Monte Carlo simulate
 a dataset with 
\begin_inset Formula $N=10^{4}$
\end_inset

 spectra without signal, and derive 
\begin_inset Formula $Z_{1}$
\end_inset

 values.
 This can be done rapidly with the presented algorithm.
 The red histogram in Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:postZ"

\end_inset

 shows the resulting Bayes factors.
 The 
\begin_inset Formula $99.9\%$
\end_inset

 quantile of 
\begin_inset Formula $B$
\end_inset

-values in this signal-free data set is 
\begin_inset Formula $B\approx10$
\end_inset

.
 Therefore, in the 
\begin_inset Quotes eld
\end_inset

real
\begin_inset Quotes erd
\end_inset

 data, those with a Bayes factor 
\begin_inset Formula $B>10$
\end_inset

 can be securely claimed to have a line, with a small fraction of false
 detection (
\begin_inset Formula $p<0.001$
\end_inset

).
\end_layout

\begin_layout Section
Application to Imaging Spectroscopy
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /mnt/data/daten/PostDoc/research/stats/massivedatans/museout/outimg_noise0.2.pdf
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:ifuresults"

\end_inset

MUSE IFU data analysed with Collaborative Nested Sampling.
 
\emph on
Top left panel
\emph default
: White image of the input data.
 Several blobs and a extended arc is visible.
 
\emph on
Top right panel
\emph default
: Bayes factor 
\begin_inset Formula $B$
\end_inset

 comparing the single-stellar population model to a no-signal model.
 In the arc and blobs, 
\begin_inset Formula $\log B>0$
\end_inset

.
 The remaining panels present posterior parameters (
\emph on
left
\emph default
) and uncertainties (
\emph on
right
\emph default
).
 Redshift 
\begin_inset Formula $z$
\end_inset

 is well-constrained across the arc (
\begin_inset Formula $z\approx0.6$
\end_inset

) and the closer blobs (
\begin_inset Formula $z\approx0.4$
\end_inset

).
 While the blobs had a brief star formation episode (
\begin_inset Formula $\log\tau/\mathrm{yr}\approx7$
\end_inset

) long ago (age > 
\begin_inset Formula $10^{10}$
\end_inset

 years), the arc shows evidence for recent star formation (younger age,
 slower decay 
\begin_inset Formula $\tau$
\end_inset

).
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Finally, I apply collaborative nested sampling to a real-world data set.
 Integral-field unit (IFU) observations, where many spectra are taken in
 proximity on the sky are ideal for applying the algorithm.
 
\end_layout

\begin_layout Standard
The top-left panel of Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ifuresults"

\end_inset

 shows the region in the sky chosen.
 The MUSE spectrograph (1arcmin² field of view, wavelength range 480-930nm;
 
\begin_inset CommandInset citation
LatexCommand cite
key "Bacon2010"
literal "true"

\end_inset

) observed the Abell
\begin_inset space ~
\end_inset

370 galaxy cluster in November 2014 (PI: Richard) for one hour
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Additional observations have been made since then, however here the demonstratio
n is intended to show information extraction in the low-signal regime.
\end_layout

\end_inset

.
 Following e.g., 
\begin_inset CommandInset citation
LatexCommand cite
key "Lagattuta2017"
literal "true"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "Patricio2018"
literal "true"

\end_inset

, standard data reduction procedures and sky line subtraction (
\begin_inset CommandInset citation
LatexCommand cite
key "Soto2016ZAP"
literal "false"

\end_inset

) were used, and the errors increased by 20% of the data value to account
 for model inaccuracies.
 The chosen region (A370-sys1 in 
\begin_inset CommandInset citation
LatexCommand cite
key "Patricio2018"
literal "true"

\end_inset

) includes several galaxies, some of which heavily distorted by strong lensing.
 The 169 arcsec² sized region is covered by 4223 fibers, each of which providing
 a spectrum with measured intensity and error.
 The white image (sum across the spectrum) is shown in the top-left panel
 of Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ifuresults"

\end_inset

.
\end_layout

\begin_layout Standard
A simple stellar population is used to model the spectra.
 The classic Bruzual & Charlot model stellar spectra 
\begin_inset CommandInset citation
LatexCommand cite
key "Bruzual2003"
literal "true"

\end_inset

 are weighed by an exponential star formation (decay timescale 
\begin_inset Formula $\tau$
\end_inset

) at time 
\begin_inset Formula $t$
\end_inset

 Gyrs in the past.
 Additionally, dust extinction is allowed through a Calzetti law
\begin_inset CommandInset citation
LatexCommand cite
key "Calzetti2000"
literal "true"

\end_inset

 and the model is redshifted.
 To avoid a degeneracy with star formation age, I assume solar metallicity.
 This model thus has four parameters: redshift 
\begin_inset Formula $z$
\end_inset

 (0-1), star formation age 
\begin_inset Formula $t$
\end_inset

 (0-13
\begin_inset space ~
\end_inset

Gyr), star formation decay time 
\begin_inset Formula $\tau$
\end_inset

 (
\begin_inset Formula $10^{6-9}$
\end_inset

 Gyr) and extinction 
\begin_inset Formula $E(B-V)$
\end_inset

 (0-1).
 Uniform priors are applied on 
\begin_inset Formula $z$
\end_inset

, 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $\log\tau$
\end_inset

 and 
\begin_inset Formula $E(B-V)$
\end_inset

.
\end_layout

\begin_layout Standard
A Gaussian likelihood compares the model spectrum 
\begin_inset Formula $M_{i}$
\end_inset

 against the measurements 
\begin_inset Formula $\mu_{i}$
\end_inset

 and errors 
\begin_inset Formula $\sigma_{i}$
\end_inset

.
 To avoid having the model normalisation 
\begin_inset Formula $s$
\end_inset

 as a fitting parameter, it is marginalised over, by setting 
\begin_inset Formula 
\begin{equation}
s=\sum_{i}\left(\mu_{i}M_{i}\sigma_{i}^{-2}\right)/\sum_{i}\left(M_{i}^{2}\sigma_{i}^{-2}\right)
\end{equation}

\end_inset

 and neglecting constants in the likelihood (see 
\begin_inset CommandInset citation
LatexCommand cite
key "Arnouts1999"
literal "true"

\end_inset

 for more details):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\log L=-\frac{1}{2}\sum\left[(\mu_{i}-s\cdot M_{i})/\sigma_{i}\right]^{2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
With the model, likelihood and data defined, I apply collaborative nested
 sampling with ellipsoidal sampling and derive posterior parameter distribution
 at each spaxel.
 evidence values are also obtained.
 Similar to the previous section, Bayes factors are computed and shown in
 the top right panel in Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ifuresults"

\end_inset

 at each spaxel.
 The red areas indicate where the data prefer no input signal over the stellar
 model.
 
\end_layout

\begin_layout Standard
The second row of Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ifuresults"

\end_inset

 shows the derived redshift.
 Uncertainties (right panel) are extremely small (typically 0.001) over most
 of the image.
 Two solutions are visible in the left panel: The arc (green) is at distinctly
 higher redshifts than the blobs (yellow).
 Extended emission at the same redshift as the blobs is detected.
\end_layout

\begin_layout Standard
The third and forth row present the star formation properties in each pixel.
 The arc shows evidence for recent star formation.
 While uncertainties on the decay parameter are not small, generally the
 arc has a longer (
\begin_inset Formula $10^{8}\,\mathrm{Gyr}$
\end_inset

) star formation episode than the blobs (
\begin_inset Formula $10^{7}\,\mathrm{Gyr}$
\end_inset

).
 The extinction is constant over image and shows small values (
\begin_inset Formula $<0.1$
\end_inset

; not shown).
 The model used here is overly simple and I do not interpret the physical
 meaning in great detail (see 
\begin_inset CommandInset citation
LatexCommand cite
key "Patricio2018"
literal "true"

\end_inset

 instead).
 In particular, the relation between metallicity and age should be explored
 further.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Patricio2018"
literal "true"

\end_inset

 however derived similar values e.g., for the star formation age and timescale
 when considering the stacked spectrum across the entire arc.
\end_layout

\begin_layout Standard
This application demonstrates the usefulness of collaborative nested sampling
 in a realistic dataset.
 Physical parameters were extracted while exploiting that spatial neighbours
 have similar physical properties.
 However, no assumption about smoothness or neighbourhood was made, i.e.,
 the constraints at each pixel are independent.
 Also note that in every pixel, a posterior distribution over the parameters
 is derived.
 Here, the collaborative nested sampling analysis of 4223 fibers required
 14.4 million likelihood evaluations (140h).
 This corresponds to a quadrupling of the efficiency compared to analysing
 only 100 fibers, which required 2.8 million likelihood evaluations (14.9h).
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
Collaborative nested sampling is a scalable algorithm suitable for analysing
 massive data sets with arbitrarily complex physical models and complex,
 inhomogeneous noise properties.
 The algorithm brings to the Big Data regime parameter estimation with uncertain
ties, classification of objects and distinction between physical processes.
\end_layout

\begin_layout Standard
The key insight in this work is to take advantage of a property specific
 to nested sampling: The sampling regions can look similar across similar
 data sets, and rejection sampling from the union of contours is permitted
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
As in classic nested sampling, the volume shrinkage estimates are valid
 on average.
 Multiple runs can test whether this leads to additional scatter in the
 integral estimate.
 In practice, single runs already give correct uncertainties for many problems.
\end_layout

\end_inset

.
 
\emph on
Collaborative nested sampling
\emph default
 reduces the number of unique model evaluations, in particular at the beginning
 of the nested sampling run.
 The same approach cannot be followed with Markov Chain proposals: There,
 the proposal depends on the current point, and deviating acceptances prohibit
 a later joint proposal.
 
\end_layout

\begin_layout Standard
The algorithm has some overhead related to the management of live points,
 in particularly to determine the unique set of live points across a dynamically
 selected subgroup of data sets.
 The memory usage also grows if big data sets have to be held in the same
 machine.
 If only chunks of 
\begin_inset Formula $N$
\end_inset

 are managable, the analyses can be split into such sizes and analysed in
 parallel across multiple machines.
 In that case, one can take advantage of the scaling of the algorithm until
 
\begin_inset Formula $N$
\end_inset

.
 
\end_layout

\begin_layout Standard
The algorithm can be applied immediately to any existing large data sets.
 Compared to other Big Data analysis approaches, nested sampling supports
 model comparison and yields full probability distributions on arbitrary
 models, allowing the exploration of degenerate fit solutions.
 Furthermore, the instrument response can be modelled out and separated
 from the process of interest.
 To give one application example, 
\emph on
eROSITA
\emph default
 
\begin_inset CommandInset citation
LatexCommand citep
key "Predehl2014eRosita"
literal "true"

\end_inset

 requires the source classification and characterisation of 3 million point
 sources in its all-sky X-ray survey 
\begin_inset CommandInset citation
LatexCommand citep
key "Kolodzig2012"
literal "true"

\end_inset

.
 The position-dependent detector response and non-Gaussianity of count data
 make standard machine learning approaches difficult to apply.
\end_layout

\begin_layout Standard
Even in the analysis of single objects the presented algorithm can help.
 One might test the correctness of selecting a more complex model, e.g., based
 on Bayes factors, as in the toy example presented.
 Large Monte Carlo simulations of a null hypothesis model can be quickly
 analysed with the presented method, with a model evaluation cost that is
 essentially independent of the number of generated data sets.
 Going further, approaches to validate models and Bayesian inference (e.g.,
 
\begin_inset CommandInset citation
LatexCommand cite
key "Talts2018"

\end_inset

) over the entire parameter space can be sped up.
\end_layout

\begin_layout Section*
Acknowledgements
\end_layout

\begin_layout Standard
I thank Surangkhana Rukdee and Frederik Beaujean for reading the manuscript,
 and Franz E.
 Bauer for help with MUSE data.
\end_layout

\begin_layout Standard
I acknowledge support from the CONICYT-Chile grants Basal-CATA PFB-06/2007,
 FONDECYT Postdoctorados 3160439 and the Ministry of Economy, Development,
 and Tourism's Millennium Science Initiative through grant IC120009, awarded
 to The Millennium Institute of Astrophysics, MAS.
 This research was supported by the DFG cluster of excellence 
\begin_inset Quotes eld
\end_inset

Origin and Structure of the Universe
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "/mnt/data/daten/PostDoc/literature/agn"
options "spmpsci"

\end_inset


\end_layout

\end_body
\end_document
